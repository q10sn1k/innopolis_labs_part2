{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:29:35.007497800Z",
     "start_time": "2023-10-17T19:29:34.995351600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, final work\n"
     ]
    }
   ],
   "source": [
    "print('hello, final work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"jokes.csv\")\n",
    "\n",
    "# –£–¥–∞–ª–∏—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# –£–¥–∞–ª–∏—Ç—å —Å—Ç—Ä–æ–∫–∏ —Å –ø—É—Å—Ç—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏\n",
    "df = df.dropna()\n",
    "\n",
    "df.to_csv(\"jokes.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:34:00.142676100Z",
     "start_time": "2023-10-17T19:33:54.591758600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('jokes.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    jokes = list(reader)\n",
    "\n",
    "\n",
    "with open('jokes_test.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(jokes[:100])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T22:46:12.645814700Z",
     "start_time": "2023-10-17T22:46:11.924231300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0      theme                                               text  \\\n0           0  pro-sudey  –ù–∞ —Å—É–¥–µ –≤ –°—Ç–∞–º–±—É–ª–µ –æ–±–≤–∏–Ω—è–µ–º—ã–∏ÃÜ —Å–∫–∞–∑–∞–ª:\\r\\n- –ù–∞...   \n1           1  pro-sudey  - –í—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç–µ —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å, —á—Ç–æ –æ–±–≤–∏–Ω—è–µ–º—ã–∏ÃÜ –Ω...   \n2           2  pro-sudey  –ù–∞ —Å—É–¥–µ.\\r\\n- –ò—Ç–∞–∫, –∫–æ–≥–¥–∞ –¥–µ–ª–æ –¥–æ—à–ª–æ –¥–æ —Å—Ç–æ–ª–∫–Ω...   \n3           3  pro-sudey  –°—Ç–∞—Ä—É—é –ª–µ–¥–∏ —Å–±–∏–ª –∞–≤—Ç–æ–º–æ–±–∏–ª—å. –ù–∞ —Å—É–¥–µ –µ–µ —Å–ø—Ä–∞—à–∏...   \n4           4  pro-sudey  –°—É–¥—å—è –≥–æ–≤–æ—Ä–∏—Ç:\\r\\n- –°–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–µ–∏ÃÜ –∂–∞–ª–æ–±–µ, –æ–±...   \n5           5  pro-sudey  –ù–∞ —Å—É–¥–µ–±–Ω–æ–º –∑–∞—Å–µ–¥–∞–Ω–∏–∏.\\r\\n- –ì—Ä–∞–∂–¥–∞–Ω–∫–∞ –î—Ä–æ–∑–¥–æ–≤–∞...   \n6           6  pro-sudey  - –î—Ä–∞–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–∞ —Ç–∞–∫. –û–¥–Ω–æ–∏ÃÜ —Ä—É–∫–æ–∏ÃÜ —è —Å—Ö–≤–∞—Ç...   \n7           7  pro-sudey  –°—É–¥—å—è:\\r\\n- –°–≤–∏–¥–µ—Ç–µ–ª—å, –≤—ã –¥–æ–ª–∂–Ω—ã –≥–æ–≤–æ—Ä–∏—Ç—å –ø—Ä–∞–≤...   \n8           8  pro-sudey  –°—É–¥—å—è –ø–æ–¥—Å—É–¥–∏–º–æ–º—É:\\r\\n- –ù—É-–Ω—É, –ø–µ—Ä–µ—Å—Ç–∞–Ω—å—Ç–µ –≤–æ–ª...   \n9           9  pro-sudey  –°—É–¥—å—è —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ—Ö –∏–Ω–¥–µ–∏ÃÜ—Ü–µ–≤, —É–±–µ–∂–∞–≤—à–∏—Ö ...   \n\n   rating  \n0       5  \n1       4  \n2       0  \n3       4  \n4       2  \n5       3  \n6      -3  \n7       2  \n8      -2  \n9       4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>theme</th>\n      <th>text</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>pro-sudey</td>\n      <td>–ù–∞ —Å—É–¥–µ –≤ –°—Ç–∞–º–±—É–ª–µ –æ–±–≤–∏–Ω—è–µ–º—ã–∏ÃÜ —Å–∫–∞–∑–∞–ª:\\r\\n- –ù–∞...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>pro-sudey</td>\n      <td>- –í—ã –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç–µ —É—Ç–≤–µ—Ä–∂–¥–∞—Ç—å, —á—Ç–æ –æ–±–≤–∏–Ω—è–µ–º—ã–∏ÃÜ –Ω...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>pro-sudey</td>\n      <td>–ù–∞ —Å—É–¥–µ.\\r\\n- –ò—Ç–∞–∫, –∫–æ–≥–¥–∞ –¥–µ–ª–æ –¥–æ—à–ª–æ –¥–æ —Å—Ç–æ–ª–∫–Ω...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>pro-sudey</td>\n      <td>–°—Ç–∞—Ä—É—é –ª–µ–¥–∏ —Å–±–∏–ª –∞–≤—Ç–æ–º–æ–±–∏–ª—å. –ù–∞ —Å—É–¥–µ –µ–µ —Å–ø—Ä–∞—à–∏...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>pro-sudey</td>\n      <td>–°—É–¥—å—è –≥–æ–≤–æ—Ä–∏—Ç:\\r\\n- –°–æ–≥–ª–∞—Å–Ω–æ –≤–∞—à–µ–∏ÃÜ –∂–∞–ª–æ–±–µ, –æ–±...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>pro-sudey</td>\n      <td>–ù–∞ —Å—É–¥–µ–±–Ω–æ–º –∑–∞—Å–µ–¥–∞–Ω–∏–∏.\\r\\n- –ì—Ä–∞–∂–¥–∞–Ω–∫–∞ –î—Ä–æ–∑–¥–æ–≤–∞...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>pro-sudey</td>\n      <td>- –î—Ä–∞–∫–∞ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏–ª–∞ —Ç–∞–∫. –û–¥–Ω–æ–∏ÃÜ —Ä—É–∫–æ–∏ÃÜ —è —Å—Ö–≤–∞—Ç...</td>\n      <td>-3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>pro-sudey</td>\n      <td>–°—É–¥—å—è:\\r\\n- –°–≤–∏–¥–µ—Ç–µ–ª—å, –≤—ã –¥–æ–ª–∂–Ω—ã –≥–æ–≤–æ—Ä–∏—Ç—å –ø—Ä–∞–≤...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>pro-sudey</td>\n      <td>–°—É–¥—å—è –ø–æ–¥—Å—É–¥–∏–º–æ–º—É:\\r\\n- –ù—É-–Ω—É, –ø–µ—Ä–µ—Å—Ç–∞–Ω—å—Ç–µ –≤–æ–ª...</td>\n      <td>-2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>pro-sudey</td>\n      <td>–°—É–¥—å—è —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç —á–µ—Ç—ã—Ä–µ—Ö –∏–Ω–¥–µ–∏ÃÜ—Ü–µ–≤, —É–±–µ–∂–∞–≤—à–∏—Ö ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:44:02.514515400Z",
     "start_time": "2023-10-17T19:44:02.493963200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "     -------------------------------------- 192.3/192.3 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (4.8.0)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:49:21.876571200Z",
     "start_time": "2023-10-17T19:47:17.191825900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pip install torch -f https://download.pytorch.org/whl/cpu/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/6 : < :, Epoch 0.33/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training Loss: 2.429222742716471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 1.902648687362671, 'eval_runtime': 2.527, 'eval_samples_per_second': 1.979, 'eval_steps_per_second': 0.396}\n",
      "Generated text 1:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã\n",
      "Generated text 2:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è, –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤\n",
      "Generated text 3:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤–∞\n",
      "Generated text 4:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è, –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã —Å\n",
      "Generated text 5:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤–æÔøΩ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "import os\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def read_jokes_csv(filename):\n",
    "    df = pd.read_csv(filename, usecols=['text'])\n",
    "    return df\n",
    "\n",
    "def train_and_save_model():\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    df = read_jokes_csv(\"jokes.csv\")\n",
    "    df = df.head(20)\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    eval_df = df.drop(train_df.index)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª—ã\n",
    "    train_path = \"jokes_train.txt\"\n",
    "    test_path = \"jokes_test.txt\"\n",
    "    train_df['text'].to_csv(train_path, index=False, header=False)\n",
    "    eval_df['text'].to_csv(test_path, index=False, header=False)\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128, overwrite_cache=True)\n",
    "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128, overwrite_cache=True)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./jokes_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        eval_steps=10,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "    training_results = trainer.train()\n",
    "    trainer.save_model(\"./jokes_model\")\n",
    "\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    return model, tokenizer, training_results.training_loss\n",
    "\n",
    "def load_and_test_model(model_path, test_path):\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ —Ñ–∞–π–ª–∞\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=test_path,\n",
    "        block_size=128,\n",
    "        overwrite_cache=True\n",
    "    )\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_test\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs_test\",\n",
    "        logging_steps=10,\n",
    "        eval_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False\n",
    "        ),\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    print(\"Evaluation Results:\", results)\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "    input_text = \"–ù–∞ —Å—É–¥–µ\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # output = model.generate(input_ids, max_length=100, num_return_sequences=5, temperature=0.7)\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=5, num_beams=5, temperature=0.7)\n",
    "    for i, generated_text in enumerate(tokenizer.decode(o, skip_special_tokens=True) for o in output):\n",
    "        print(f\"Generated text {i + 1}:\")\n",
    "        print(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    model, tokenizer, average_loss = train_and_save_model()\n",
    "\n",
    "    # –í—ã–≤–æ–¥ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ loss –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏\n",
    "    print(f\"Avg Training Loss: {average_loss}\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    load_and_test_model(\"./jokes_model\", \"jokes_test.txt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/6 : < :, Epoch 0.33/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training Loss: 2.429222742716471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 1.902648687362671, 'eval_runtime': 2.3857, 'eval_samples_per_second': 2.096, 'eval_steps_per_second': 0.419}\n",
      "Generated text 1:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã\n",
      "Generated text 2:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è, –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤\n",
      "Generated text 3:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤–∞\n",
      "Generated text 4:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è, –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã —Å\n",
      "Generated text 5:\n",
      "–ù–∞ —Å—É–¥–µ–ª—å—Å—Ç–≤—É—é—Ç—Å—è –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤—ã –≤–æÔøΩ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "import os\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def read_jokes_csv(filename):\n",
    "    df = pd.read_csv(filename, usecols=['text'])\n",
    "    return df\n",
    "\n",
    "def train_and_save_model():\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    df = read_jokes_csv(\"jokes.csv\")\n",
    "    df = df.head(20)\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    eval_df = df.drop(train_df.index)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª—ã\n",
    "    train_path = \"jokes_train.txt\"\n",
    "    test_path = \"jokes_test.txt\"\n",
    "    train_df['text'].to_csv(train_path, index=False, header=False)\n",
    "    eval_df['text'].to_csv(test_path, index=False, header=False)\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128, overwrite_cache=True)\n",
    "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128, overwrite_cache=True)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./jokes_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=2,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        eval_steps=10,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "    training_results = trainer.train()\n",
    "    trainer.save_model(\"./jokes_model\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    tokenizer.save_pretrained(\"./jokes_model\")\n",
    "\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    return model, tokenizer, training_results.training_loss\n",
    "\n",
    "def load_and_test_model(model_path, test_path):\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ —Ñ–∞–π–ª–∞\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=test_path,\n",
    "        block_size=128,\n",
    "        overwrite_cache=True\n",
    "    )\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_test\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs_test\",\n",
    "        logging_steps=10,\n",
    "        eval_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False\n",
    "        ),\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    print(\"Evaluation Results:\", results)\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "    input_text = \"–ù–∞ —Å—É–¥–µ\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # output = model.generate(input_ids, max_length=100, num_return_sequences=5, temperature=0.7)\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=5, num_beams=5, temperature=0.7)\n",
    "    for i, generated_text in enumerate(tokenizer.decode(o, skip_special_tokens=True) for o in output):\n",
    "        print(f\"Generated text {i + 1}:\")\n",
    "        print(generated_text)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    model, tokenizer, average_loss = train_and_save_model()\n",
    "\n",
    "    # –í—ã–≤–æ–¥ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ loss –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏\n",
    "    print(f\"Avg Training Loss: {average_loss}\")\n",
    "\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    load_and_test_model(\"./jokes_model\", \"jokes_test.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:19:25.979615300Z",
     "start_time": "2023-10-19T15:17:46.843567100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "import os\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def read_jokes_csv(filename):\n",
    "    df = pd.read_csv(filename, usecols=['text'])\n",
    "    return df\n",
    "\n",
    "def train_and_save_model():\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    df = read_jokes_csv(\"jokes.csv\")\n",
    "    df = df.head(100)\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    eval_df = df.drop(train_df.index)\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–∞–π–ª—ã\n",
    "    train_path = \"jokes_train.txt\"\n",
    "    test_path = \"jokes_test.txt\"\n",
    "    train_df['text'].to_csv(train_path, index=False, header=False)\n",
    "    eval_df['text'].to_csv(test_path, index=False, header=False)\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128, overwrite_cache=True)\n",
    "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128, overwrite_cache=True)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./jokes_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        eval_steps=10,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "    training_results = trainer.train()\n",
    "    trainer.save_model(\"./jokes_model\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
    "    tokenizer.save_pretrained(\"./jokes_model\")\n",
    "\n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—å –¥–ª—è –≤—Å–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    return model, tokenizer, training_results.training_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:21:26.523947300Z",
     "start_time": "2023-10-19T15:21:25.787168500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_and_test_model(model_path, test_path):\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏–∑ —Ñ–∞–π–ª–∞\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=test_path,\n",
    "        block_size=128,\n",
    "        overwrite_cache=True\n",
    "    )\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_test\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs_test\",\n",
    "        logging_steps=10,\n",
    "        eval_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False\n",
    "        ),\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    print(\"Evaluation Results:\", results)\n",
    "\n",
    "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏\n",
    "    input_text = \"–ü—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # output = model.generate(input_ids, max_length=100, num_return_sequences=5, temperature=0.7)\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=5, num_beams=5, temperature=0.7)\n",
    "    for i, generated_text in enumerate(tokenizer.decode(o, skip_special_tokens=True) for o in output):\n",
    "        print(f\"Generated text {i + 1}:\")\n",
    "        print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:21:59.890383600Z",
     "start_time": "2023-10-19T15:21:59.876383500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/72 : < :, Epoch 0.06/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training Loss: 2.1047313610712686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/5 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 2.029554605484009, 'eval_runtime': 17.3146, 'eval_samples_per_second': 2.021, 'eval_steps_per_second': 0.289}\n",
      "Generated text 1:\n",
      "–ù–∞ —Å—É–¥–µ–Ω—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç –ø—Ä–∏—Ö–æ–¥–∏—Ç\n",
      "Generated text 2:\n",
      "–ù–∞ —Å—É–¥–µ–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ ÔøΩ\n",
      "Generated text 3:\n",
      "–ù–∞ —Å—É–¥–µ–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ.\n",
      "Generated text 4:\n",
      "–ù–∞ —Å—É–¥–µ–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ\n",
      "\n",
      "Generated text 5:\n",
      "–ù–∞ —Å—É–¥–µ–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ –ø—Ä–∏—Ö–æ–¥–∏ÃÜ \n"
     ]
    }
   ],
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model, tokenizer, average_loss = train_and_save_model()\n",
    "\n",
    "print(f\"Avg Training Loss: {average_loss}\")\n",
    "\n",
    "load_and_test_model(\"./jokes_model\", \"jokes_test.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T16:29:15.618069400Z",
     "start_time": "2023-10-19T16:10:47.896177500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
