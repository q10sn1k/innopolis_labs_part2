{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:29:35.007497800Z",
     "start_time": "2023-10-17T19:29:34.995351600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, final work\n"
     ]
    }
   ],
   "source": [
    "print('hello, final work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"jokes.csv\")\n",
    "\n",
    "# Удалить дубликаты\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Удалить строки с пустыми значениями\n",
    "df = df.dropna()\n",
    "\n",
    "df.to_csv(\"jokes.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:34:00.142676100Z",
     "start_time": "2023-10-17T19:33:54.591758600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('jokes.csv', 'r', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    jokes = list(reader)\n",
    "\n",
    "\n",
    "with open('jokes_test.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(jokes[:100])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T22:46:12.645814700Z",
     "start_time": "2023-10-17T22:46:11.924231300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "   Unnamed: 0      theme                                               text  \\\n0           0  pro-sudey  На суде в Стамбуле обвиняемый сказал:\\r\\n- На...   \n1           1  pro-sudey  - Вы продолжаете утверждать, что обвиняемый н...   \n2           2  pro-sudey  На суде.\\r\\n- Итак, когда дело дошло до столкн...   \n3           3  pro-sudey  Старую леди сбил автомобиль. На суде ее спраши...   \n4           4  pro-sudey  Судья говорит:\\r\\n- Согласно вашей жалобе, об...   \n5           5  pro-sudey  На судебном заседании.\\r\\n- Гражданка Дроздова...   \n6           6  pro-sudey  - Драка происходила так. Одной рукой я схват...   \n7           7  pro-sudey  Судья:\\r\\n- Свидетель, вы должны говорить прав...   \n8           8  pro-sudey  Судья подсудимому:\\r\\n- Ну-ну, перестаньте вол...   \n9           9  pro-sudey  Судья спрашивает четырех индейцев, убежавших ...   \n\n   rating  \n0       5  \n1       4  \n2       0  \n3       4  \n4       2  \n5       3  \n6      -3  \n7       2  \n8      -2  \n9       4  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>theme</th>\n      <th>text</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>pro-sudey</td>\n      <td>На суде в Стамбуле обвиняемый сказал:\\r\\n- На...</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>pro-sudey</td>\n      <td>- Вы продолжаете утверждать, что обвиняемый н...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>pro-sudey</td>\n      <td>На суде.\\r\\n- Итак, когда дело дошло до столкн...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>pro-sudey</td>\n      <td>Старую леди сбил автомобиль. На суде ее спраши...</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>pro-sudey</td>\n      <td>Судья говорит:\\r\\n- Согласно вашей жалобе, об...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>pro-sudey</td>\n      <td>На судебном заседании.\\r\\n- Гражданка Дроздова...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>pro-sudey</td>\n      <td>- Драка происходила так. Одной рукой я схват...</td>\n      <td>-3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>pro-sudey</td>\n      <td>Судья:\\r\\n- Свидетель, вы должны говорить прав...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>pro-sudey</td>\n      <td>Судья подсудимому:\\r\\n- Ну-ну, перестаньте вол...</td>\n      <td>-2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>pro-sudey</td>\n      <td>Судья спрашивает четырех индейцев, убежавших ...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:44:02.514515400Z",
     "start_time": "2023-10-17T19:44:02.493963200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.1.0-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "     -------------------------------------- 192.3/192.3 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (4.8.0)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: jinja2 in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\data\\innopolis\\itog\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, torch\n",
      "Successfully installed mpmath-1.3.0 networkx-3.1 sympy-1.12 torch-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-17T19:49:21.876571200Z",
     "start_time": "2023-10-17T19:47:17.191825900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# pip install torch -f https://download.pytorch.org/whl/cpu/torch_stable.html"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "import os\n",
    "\n",
    "# Определение токенизатора\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def read_jokes_csv(filename):\n",
    "    df = pd.read_csv(filename, usecols=['text'])\n",
    "    return df\n",
    "\n",
    "def train_and_save_model():\n",
    "    # Загрузка данных\n",
    "    df = read_jokes_csv(\"jokes.csv\")\n",
    "    df = df.head(100)\n",
    "    train_df = df.sample(frac=0.8, random_state=42)\n",
    "    eval_df = df.drop(train_df.index)\n",
    "\n",
    "    # Сохранение данных в файлы\n",
    "    train_path = \"jokes_train.txt\"\n",
    "    test_path = \"jokes_test.txt\"\n",
    "    train_df['text'].to_csv(train_path, index=False, header=False)\n",
    "    eval_df['text'].to_csv(test_path, index=False, header=False)\n",
    "\n",
    "    # Токенизация данных\n",
    "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=train_path, block_size=128, overwrite_cache=True)\n",
    "    test_dataset = TextDataset(tokenizer=tokenizer, file_path=test_path, block_size=128, overwrite_cache=True)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    # Определение параметров обучения\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./jokes_model\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=4,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=10,\n",
    "        eval_steps=10,\n",
    "        save_total_limit=2,\n",
    "        evaluation_strategy=\"steps\"\n",
    "    )\n",
    "\n",
    "    # Обучение модели\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # Тренировка модели\n",
    "    training_results = trainer.train()\n",
    "    trainer.save_model(\"./jokes_model\")\n",
    "\n",
    "    # Сохранение токенизатора\n",
    "    tokenizer.save_pretrained(\"./jokes_model\")\n",
    "\n",
    "    # Возвращаем среднее значение потерь для всего обучения\n",
    "    return model, tokenizer, training_results.training_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:21:26.523947300Z",
     "start_time": "2023-10-19T15:21:25.787168500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def load_and_test_model(model_path, test_path):\n",
    "    # Загрузка модели и токенизатора из файла\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Токенизация тестовых данных\n",
    "    test_dataset = TextDataset(\n",
    "        tokenizer=tokenizer,\n",
    "        file_path=test_path,\n",
    "        block_size=128,\n",
    "        overwrite_cache=True\n",
    "    )\n",
    "\n",
    "    # Оценка модели на тестовых данных\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results_test\",\n",
    "        per_device_eval_batch_size=8,\n",
    "        logging_dir=\"./logs_test\",\n",
    "        logging_steps=10,\n",
    "        eval_steps=10,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer, mlm=False\n",
    "        ),\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    results = trainer.evaluate()\n",
    "\n",
    "    print(\"Evaluation Results:\", results)\n",
    "\n",
    "    # Генерация текста на основе обученной модели\n",
    "    input_text = \"Программист\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    # output = model.generate(input_ids, max_length=100, num_return_sequences=5, temperature=0.7)\n",
    "    output = model.generate(input_ids, max_length=100, num_return_sequences=5, num_beams=5, temperature=0.7)\n",
    "    for i, generated_text in enumerate(tokenizer.decode(o, skip_special_tokens=True) for o in output):\n",
    "        print(f\"Generated text {i + 1}:\")\n",
    "        print(generated_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:21:59.890383600Z",
     "start_time": "2023-10-19T15:21:59.876383500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='72' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/72 : < :, Epoch 0.06/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Training Loss: 2.1047313610712686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/5 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\innopolis\\itog\\venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 2.029554605484009, 'eval_runtime': 17.3146, 'eval_samples_per_second': 2.021, 'eval_steps_per_second': 0.289}\n",
      "Generated text 1:\n",
      "На судент приходит приходит приходит приходит приходит приходит приходит приходит приходит\n",
      "Generated text 2:\n",
      "На судей приходй приходй приходй приходй приходй приходй приходй приходй �\n",
      "Generated text 3:\n",
      "На судей приходй приходй приходй приходй приходй приходй приходй приходй.\n",
      "Generated text 4:\n",
      "На судей приходй приходй приходй приходй приходй приходй приходй приходй\n",
      "\n",
      "Generated text 5:\n",
      "На судей приходй приходй приходй приходй приходй приходй приходй приходй \n"
     ]
    }
   ],
   "source": [
    "# Обучение и сохранение модели\n",
    "model, tokenizer, average_loss = train_and_save_model()\n",
    "\n",
    "print(f\"Avg Training Loss: {average_loss}\")\n",
    "\n",
    "load_and_test_model(\"./jokes_model\", \"jokes_test.txt\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T16:29:15.618069400Z",
     "start_time": "2023-10-19T16:10:47.896177500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
